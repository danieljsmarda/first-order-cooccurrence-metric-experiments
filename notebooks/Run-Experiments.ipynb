{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import filter_terms_not_in_wemodel, \\\n",
    "    get_2ndorder_association_metric_list_for_target_list, \\\n",
    "    get_1storder_association_metric_list_for_target_list, \\\n",
    "    get_expSG_1storder_relation_no_cache_NEW, \\\n",
    "    get_expSG_1storder_relation_no_cache_NEW_ALLWORDS, \\\n",
    "    get_matrices_from_term_lists, \\\n",
    "    save_arrays, open_pickle, save_pickle, \\\n",
    "    save_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n",
      "Total words: 312425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Caliskan GloVe\\nglove_file = '../data/external/glove.6B/glove.6B.50d.txt'\\n_ = glove2word2vec(glove_file, '../data/interim/tmp.txt')\\nwe_model = KeyedVectors.load_word2vec_format('../data/interim/tmp.txt')\\nprint('loading done!')\\nprint(f'Total words: {len(we_model.wv.vocab)}')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we_model_name = \"sg_dim300_min100_win5\"\n",
    "we_vector_size = 300\n",
    "we_model_dir = '../data/external/wiki-english/wiki-english-20171001/%s' % we_model_name\n",
    "\n",
    "we_model = Word2Vec.load(we_model_dir+'/model.gensim')\n",
    "print ('loading done!')\n",
    "print(f'Total words: {len(we_model.wv.vocab)}')\n",
    "\n",
    "'''\n",
    "# Caliskan GloVe\n",
    "glove_file = '../data/external/glove.6B/glove.6B.50d.txt'\n",
    "_ = glove2word2vec(glove_file, '../data/interim/tmp.txt')\n",
    "we_model = KeyedVectors.load_word2vec_format('../data/interim/tmp.txt')\n",
    "print('loading done!')\n",
    "print(f'Total words: {len(we_model.wv.vocab)}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = '../data/interim/association_metric_exps.pickle'\n",
    "EXPERIMENT_DEFINITION_PATH = '../data/interim/experiment_definitions.pickle'\n",
    "THRESHOLD_BIASES_PATH_2NDORDER = '../data/processed/threshold_biases_2ndorder.pickle'\n",
    "THRESHOLD_BIASES_PATH_1STORDER = '../data/processed/threshold_biases_1storder.pickle'\n",
    "SCALERS_FILEPATH = '../data/processed/scalers.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_experiment_definition(exp_num, X_terms, Y_terms, A_terms, B_terms, X_label, Y_label, A_label, B_label, filepath):\n",
    "    dct = open_pickle(filepath)\n",
    "    dct[exp_num]['X_terms'] = X_terms\n",
    "    dct[exp_num]['Y_terms'] = Y_terms\n",
    "    dct[exp_num]['A_terms'] = A_terms\n",
    "    dct[exp_num]['B_terms'] = B_terms\n",
    "    dct[exp_num]['X_label'] = X_label\n",
    "    dct[exp_num]['Y_label'] = Y_label\n",
    "    dct[exp_num]['A_label'] = A_label\n",
    "    dct[exp_num]['B_label'] = B_label\n",
    "    save_pickle(dct, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following terms were removed from the list first_list because they were not found in the we_model: ['gladiola']\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n",
      "The following terms were removed from the second list to balance the length of the lists: ['ant']\n",
      "The following terms were removed from the list first_list because they were not found in the we_model: []\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n"
     ]
    }
   ],
   "source": [
    "# WEAT 1\n",
    "exp_num = 1\n",
    "X_label = 'Flowers'\n",
    "Y_label = 'Insects'\n",
    "A_label = 'Pleasant'\n",
    "B_label = 'Unpleasant'\n",
    "X_terms = ['aster', 'clover', 'hyacinth', 'marigold', 'poppy', 'azalea', \n",
    "           'crocus', 'iris', 'orchid', 'rose', 'bluebell', 'daffodil','lilac',\n",
    "           'pansy','tulip','buttercup','daisy','lily','penny','violet','carnation', 'gladiola',\n",
    "           'magnolia','petunia','zinnia']\n",
    "Y_terms = ['ant','caterpillar','flea','locust','spider','bedbug','centipede','fly',\n",
    "          'maggot','tarantula','bee','cockroach','gnat','mosquito','termite','beetle',\n",
    "          'cricket','hornet','moth','wasp','blackfly','dragonfly','horsefly','roach',\n",
    "          'weevil']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "           'loyal','pleasure','diamond','gentle','honest','lucky','rainbow','diploma',\n",
    "           'gift','honor','miracle','sunrise','family','happy','laugher','paradise',\n",
    "           'vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy',\n",
    "          'divorce','jail','poverty','ugly','cancer','kill','rotten','vomit','agony',\n",
    "          'prison']\n",
    "X_terms, Y_terms = filter_terms_not_in_wemodel(we_model, X_terms, Y_terms)\n",
    "A_terms, B_terms = filter_terms_not_in_wemodel(we_model, A_terms, B_terms)\n",
    "add_experiment_definition(exp_num, X_terms, Y_terms, A_terms, B_terms, \n",
    "                          X_label, Y_label, A_label, B_label, EXPERIMENT_DEFINITION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following terms were removed from the list first_list because they were not found in the we_model: []\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n",
      "The following terms were removed from the second list to balance the length of the lists: ['arrow']\n",
      "The following terms were removed from the list first_list because they were not found in the we_model: []\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n"
     ]
    }
   ],
   "source": [
    "# WEAT 2\n",
    "exp_num = 2\n",
    "X_label = 'Instruments'\n",
    "Y_label = 'Weapons'\n",
    "A_label = 'Pleasant'\n",
    "B_label = 'Unpleasant'\n",
    "X_terms = ['bagpipe','cello','guitar','lute','trombone','banjo','clarinet','harmonica',\n",
    "           'mandolin','trumpet','bassoon','drum','harp','oboe','tuba','bell','fiddle',\n",
    "           'harpsichord','piano','viola','bongo','flute','horn','saxophone']\n",
    "Y_terms = ['arrow','club','gun','missile','spear','axe','dagger','harpoon','pistol',\n",
    "          'sword','blade','dynamite','hatchet','rifle','tank','bomb','firearm',\n",
    "          'knife','shotgun','teargas','cannon','grenade','mace','slingshot','whip']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "           'loyal','pleasure','diamond','gentle','honest','lucky','rainbow','diploma',\n",
    "           'gift','honor','miracle','sunrise','family','happy','laugher','paradise',\n",
    "           'vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy',\n",
    "          'divorce','jail','poverty','ugly','cancer','kill','rotten','vomit','agony',\n",
    "          'prison']\n",
    "X_terms, Y_terms = filter_terms_not_in_wemodel(we_model, X_terms, Y_terms)\n",
    "A_terms, B_terms = filter_terms_not_in_wemodel(we_model, A_terms, B_terms)\n",
    "add_experiment_definition(exp_num, X_terms, Y_terms, A_terms, B_terms, \n",
    "                          X_label, Y_label, A_label, B_label, EXPERIMENT_DEFINITION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "[ 0.06463376  0.13652503 -0.04676503  0.11253601  0.04916599  0.17652243\n",
      "  0.09883526  0.04855525  0.07594025  0.16368937  0.08388424  0.11431959\n",
      "  0.13489187  0.08018041  0.1137642   0.09362984  0.10222384  0.09800541\n",
      "  0.14387575  0.04530728  0.09423137  0.14132568  0.06971878  0.06853676]\n",
      "[-0.02787289 -0.01819867 -0.06967109 -0.01150721 -0.1667692  -0.06400925\n",
      "  0.04445949 -0.18263587 -0.07073748  0.11490756 -0.09649387 -0.04441088\n",
      " -0.11249268 -0.08999455 -0.10317296  0.02992854  0.0309861  -0.02143282\n",
      " -0.06515384  0.02837816  0.04654327  0.03567371  0.03692234 -0.11014193]\n",
      "mean bias to X 0.09431389\n",
      "mean bias to Y -0.036954004\n",
      "Bias threshold 0.0380725\n",
      "5th percentile 0.006891302671283487\n",
      "95th percentile 0.557002368569374\n",
      "Results array successfully saved to file ../data/interim/association_metric_exps.pickle under keys [1][second]\n"
     ]
    }
   ],
   "source": [
    "def calculate_cosines_for_target_word_unscaled(word_vec, A_mtx, B_mtx):\n",
    "    A_dot_v = np.dot(A_mtx, word_vec)\n",
    "    B_dot_v = np.dot(B_mtx, word_vec)\n",
    "    A_norms = np.multiply(np.linalg.norm(A_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    B_norms = np.multiply(np.linalg.norm(B_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    A_cosines = np.divide(A_dot_v, A_norms)\n",
    "    B_cosines = np.divide(B_dot_v, B_norms)\n",
    "    return np.mean(A_cosines), np.mean(B_cosines)\n",
    "\n",
    "def calculate_cosines_for_all_words_unscaled(we_model, A_mtx, B_mtx):\n",
    "    '''Computes the association metric, s(w,A,B).\n",
    "    word_vec: 1-D word vector\n",
    "    A_mtx, B_mtx: 2-D word vector arrays'''\n",
    "    #A_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, A_mtx)\n",
    "    #B_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, B_mtx)\n",
    "    A_mtx_norm = A_mtx/np.linalg.norm(A_mtx, axis=1).reshape(-1,1)\n",
    "    B_mtx_norm = B_mtx/np.linalg.norm(B_mtx, axis=1).reshape(-1,1)\n",
    "    all_mtx_norm = we_model.wv.vectors/np.linalg.norm(we_model.wv.vectors, axis=1).reshape(-1,1)\n",
    "    \n",
    "    all_associations_to_A = np.dot(A_mtx_norm, all_mtx_norm.T)\n",
    "    all_associations_to_B = np.dot(B_mtx_norm, all_mtx_norm.T)\n",
    "    \n",
    "    return np.mean(all_associations_to_A, axis=0), np.mean(all_associations_to_B, axis=0)\n",
    "\n",
    "\n",
    "def get_2ndorder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model, exp_num):\n",
    "    \n",
    "    [X_mtx, _, A_mtx, B_mtx] = get_matrices_from_term_lists(we_model, target_list, target_list, A_terms, B_terms)\n",
    "    \n",
    "    # A_associations, B_associations are associations for all words    \n",
    "    A_associations, B_associations = calculate_cosines_for_all_words_unscaled(we_model, A_mtx, B_mtx)\n",
    "    \n",
    "    \n",
    "    all_associations = np.concatenate((A_associations, B_associations))\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(all_associations.reshape(-1,1))\n",
    "    save_scalers(SCALERS_FILEPATH, exp_num, 'second', scaler)\n",
    "    \n",
    "    _th = np.mean(np.abs(A_associations - B_associations))\n",
    "    _th = scaler.transform(_th.reshape(-1, 1))[0,0]\n",
    "               \n",
    "    threshold_biases = open_pickle(THRESHOLD_BIASES_PATH_2NDORDER)\n",
    "    threshold_biases = scaler.transform(threshold_biases.reshape(-1,1))\n",
    "    pct_5 = np.percentile(threshold_biases, 5)\n",
    "    pct_95 = np.percentile(threshold_biases, 95)\n",
    "    \n",
    "    biases = A_associations - B_associations\n",
    "    biases = scaler.transform(biases.reshape(-1, 1))\n",
    "    lower_bound = np.percentile(biases, 5)\n",
    "    print(f'Lower bound: {lower_bound}')\n",
    "    upper_bound = np.percentile(biases, 95)\n",
    "    print(f'Upper bound: {upper_bound}')\n",
    "    \n",
    "    target_associations = np.apply_along_axis(lambda x_vec: calculate_cosines_for_target_word_unscaled(x_vec, A_mtx, B_mtx), 1, X_mtx)\n",
    "    \n",
    "    target_biases = []\n",
    "    A_biases = []\n",
    "    for _assoc in target_associations:\n",
    "        _A_assoc = scaler.transform(_assoc[0].reshape(-1, 1))[0,0]\n",
    "        _B_assoc = scaler.transform(_assoc[1].reshape(-1, 1))[0,0]\n",
    "        _bias = _A_assoc - _B_assoc\n",
    "        target_biases.append(_bias)\n",
    "        A_biases.append(_A_assoc)\n",
    "    return np.array(target_biases), _th, pct_5, pct_95, np.array(A_biases), lower_bound, upper_bound\n",
    "\n",
    "def run_exps_2ndorder(X_terms, Y_terms, A_terms, B_terms, exp_num):\n",
    "    order='second'\n",
    "    X_metrics, _th, pct_5, pct_95, A_biases, lower_bound, upper_bound = get_2ndorder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model, exp_num)\n",
    "    Y_metrics, _th, pct_5, pct_95, A_biases, lower_bound, upper_bound = get_2ndorder_association_metric_list_for_target_list(Y_terms, A_terms, B_terms, we_model, exp_num)\n",
    "    print (X_metrics)\n",
    "    print (Y_metrics)\n",
    "\n",
    "    print ('mean bias to X', np.mean(X_metrics))\n",
    "    print ('mean bias to Y', np.mean(Y_metrics))\n",
    "\n",
    "    print ('Bias threshold', _th)\n",
    "    print ('5th percentile', pct_5)\n",
    "    print ('95th percentile', pct_95)\n",
    "\n",
    "    order = 'second'\n",
    "    threshold = _th\n",
    "    save_arrays(FILEPATH, exp_num, order, X_metrics, Y_metrics, threshold, pct_5, pct_95, A_biases, lower_bound, upper_bound)\n",
    "run_exps_2ndorder(X_terms, Y_terms, A_terms, B_terms, exp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORDER = second\n",
      "Experiment: 1\n",
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "[ 0.06463376  0.13652503 -0.04676503  0.11253601  0.04916599  0.17652243\n",
      "  0.09883526  0.04855525  0.07594025  0.16368937  0.08388424  0.11431959\n",
      "  0.13489187  0.08018041  0.1137642   0.09362984  0.10222384  0.09800541\n",
      "  0.14387575  0.04530728  0.09423137  0.14132568  0.06971878  0.06853676]\n",
      "[-0.02787289 -0.01819867 -0.06967109 -0.01150721 -0.1667692  -0.06400925\n",
      "  0.04445949 -0.18263587 -0.07073748  0.11490756 -0.09649387 -0.04441088\n",
      " -0.11249268 -0.08999455 -0.10317296  0.02992854  0.0309861  -0.02143282\n",
      " -0.06515384  0.02837816  0.04654327  0.03567371  0.03692234 -0.11014193]\n",
      "mean bias to X 0.09431389\n",
      "mean bias to Y -0.036954004\n",
      "Bias threshold 0.0380725\n",
      "5th percentile 0.006891302671283487\n",
      "95th percentile 0.557002368569374\n",
      "Results array successfully saved to file ../data/interim/association_metric_exps.pickle under keys [1][second]\n",
      "Experiment: 2\n",
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "Lower bound: -0.12051583379507065\n",
      "Upper bound: 0.10539177060127258\n",
      "[0.06205934 0.09165296 0.0919002  0.11380053 0.09113181 0.11897811\n",
      " 0.10779971 0.08894435 0.11067972 0.14237365 0.09621373 0.1135487\n",
      " 0.14185229 0.06471875 0.08086327 0.15060195 0.12328166 0.13162217\n",
      " 0.18247157 0.08507252 0.04209799 0.12344435 0.08338574 0.09611011]\n",
      "[ 0.20805708 -0.08329672 -0.09894511  0.01462591 -0.12913164 -0.04405615\n",
      " -0.02649808 -0.0268372   0.05020308 -0.01089379 -0.04618579 -0.09935915\n",
      " -0.03268984 -0.01189023 -0.25332013 -0.15611374 -0.1442793  -0.12644878\n",
      " -0.21717617  0.0141916  -0.12324518  0.01065516  0.03379461  0.03538892]\n",
      "mean bias to X 0.105608545\n",
      "mean bias to Y -0.052643776\n",
      "Bias threshold 0.0380725\n",
      "5th percentile 0.006891302671283487\n",
      "95th percentile 0.557002368569374\n",
      "Results array successfully saved to file ../data/interim/association_metric_exps.pickle under keys [2][second]\n",
      "Experiment: 3\n",
      "Lower bound: -0.11998455822467803\n",
      "Upper bound: 0.10168652534484854\n"
     ]
    }
   ],
   "source": [
    "def run_all_exps(order='second'):\n",
    "    exps = open_pickle(EXPERIMENT_DEFINITION_PATH)\n",
    "    print(f'ORDER = {order}')\n",
    "    for exp_num, exp in exps.items():\n",
    "        print(f'Experiment: {exp_num}')\n",
    "        X_terms = exp['X_terms']\n",
    "        Y_terms = exp['Y_terms']\n",
    "        A_terms = exp['A_terms']\n",
    "        B_terms = exp['B_terms']\n",
    "        if order == 'second':\n",
    "            run_exps_2ndorder(X_terms, Y_terms, A_terms, B_terms, exp_num)\n",
    "        else:\n",
    "            run_exps_1storder(X_terms, Y_terms, A_terms, B_terms, exp_num)\n",
    "run_all_exps(order='second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
