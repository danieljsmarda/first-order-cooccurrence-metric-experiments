{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import get_matrices_from_term_lists, produce_2ndorder_effect_size, produce_1storder_effect_size_unnormalized\n",
    "from models import produce_2ndorder_p_value, produce_1storder_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_model_name = \"sg_dim300_min100_win5\"\n",
    "we_vector_size = 300\n",
    "we_model_dir = '../data/external/wiki-english/wiki-english-20171001/%s' % we_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n"
     ]
    }
   ],
   "source": [
    "we_model = Word2Vec.load(we_model_dir+'/model.gensim')\n",
    "\n",
    "print ('loading done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading E_ctx_vec\n",
    "with open(we_model_dir+'/E_ctx_vec.pkl', 'rb') as fr:\n",
    "    E_ctx_vec = pickle.load(fr)\n",
    "with open(we_model_dir+'/E_wrd_vec.pkl', 'rb') as fr:\n",
    "    E_wrd_vec = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating *d*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 1\n",
    "X_terms = ['aster', 'clover', 'hyacinth', 'marigold', 'poppy', 'azalea', \n",
    "           'crocus', 'iris', 'orchid', 'rose', 'bluebell', 'daffodil','lilac',\n",
    "           'pansy','tulip','buttercup','daisy','lily','penny','violet','carnation',\n",
    "           'magnolia','petunia','zinnia']\n",
    "Y_terms = ['caterpillar','flea','locust','spider','bedbug','centipede','fly',\n",
    "          'maggot','tarantula','bee','cockroach','gnat','mosquito','termite','beetle',\n",
    "          'cricket','hornet','moth','wasp','blackfly','dragonfly','horsefly','roach',\n",
    "          'weevil']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "           'loyal','pleasure','diamond','gentle','honest','lucky','rainbow','diploma',\n",
    "           'gift','honor','miracle','sunrise','family','happy','laugher','paradise',\n",
    "           'vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy',\n",
    "          'divorce','jail','poverty','ugly','cancer','kill','rotten','vomit','agony',\n",
    "          'prison']\n",
    "# Removed words:\n",
    "# X: gladiola (not found)\n",
    "# Y: ant (random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 2\n",
    "X_terms = ['bagpipe','cello','guitar','lute','trombone','banjo','clarinet','harmonica',\n",
    "           'mandolin','trumpet','bassoon','drum','harp','oboe','tuba','bell','fiddle',\n",
    "           'harpsichord','piano','viola','bongo','flute','horn','saxophone']\n",
    "Y_terms = ['arrow','club','gun','missile','spear','axe','dagger','harpoon','pistol',\n",
    "          'sword','blade','dynamite','hatchet','rifle','tank','bomb','firearm',\n",
    "          'knife','shotgun','teargas','cannon','grenade','mace','slingshot','whip']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "           'loyal','pleasure','diamond','gentle','honest','lucky','rainbow','diploma',\n",
    "           'gift','honor','miracle','sunrise','family','happy','laugher','paradise',\n",
    "           'vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy',\n",
    "          'divorce','jail','poverty','ugly','cancer','kill','rotten','vomit','agony',\n",
    "          'prison']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing x terms:  ['bobbie-sue', 'sue-ellen']\n",
      "missing y terms:  ['terryl', 'aiesha', 'lashelle', 'temeka', 'tameisha', 'teretha', 'laronya', 'shanise', 'lakisha', 'sharise', 'tashika', 'lashandra', 'shavonn']\n"
     ]
    }
   ],
   "source": [
    "# WEAT 3\n",
    "X_terms = ['roger','alan','frank','ian','justin',\n",
    "          'ryan','andrew','fred','jack','matthew','stephen','brad','greg','jed',\n",
    "          'paul','todd','brandon','hank','jonathan','peter','wilbur','amanda',\n",
    "          'courtney','heather','melanie','sara','amber','crystal','katie',\n",
    "          'meredith','shannon','betsy','donna','kristin','nancy','stephanie',\n",
    "          'bobbie-sue','ellen','lauren','peggy','sue-ellen','colleen','emily',\n",
    "          'megan','rachel','wendy']\n",
    "Y_terms = ['alonzo','jamel','lerone','theo','alphonse','jerome','leroy',\n",
    "           'torrance','darnell','lamar','lionel','rashaun','tyree','deion',\n",
    "          'lamont','malik','terrence','tyrone','lavon','terryl',\n",
    "          'wardell','aiesha','lashelle','nichelle','shereen','temeka','ebony',\n",
    "          'latisha','shaniqua','tameisha','teretha','jasmine','laronya','shanise',\n",
    "          'tanisha','tia','lakisha','latoya','sharise','tashika','yolanda',\n",
    "          'lashandra','malika','shavonn','tawanda','yvette']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "          'loyal','pleasure','diamond','gentle','honest','lucky','rainbow',\n",
    "          'diploma','gift','honor','miracle','sunrise','family','happy','laughter',\n",
    "          'paradise','vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy','bomb',\n",
    "          'divorce','jail','poverty','ugly','cancer','evil','kill','rotten','vomit']\n",
    "print (\"Missing x terms: \", [x for x in X_terms if x not in we_model.wv])\n",
    "print('missing y terms: ', [x for x in Y_terms if x not in we_model.wv])\n",
    "# Removed terms\n",
    "# X: adam (random), chip (random), harry (random), josh (random)\n",
    "# Y: percell (not found), rasaan (not found), everol (not found), marcelius (not found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 4\n",
    "X_terms = ['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil',\n",
    "'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith',\n",
    "           'Sarah']\n",
    "Y_terms = ['Darnell', 'Hakim', 'Jermaine', 'Kareem', 'Jamal', 'Leroy', 'Rasheed',\n",
    "'Tremayne', 'Tyrone', 'Aisha', 'Ebony', 'Keisha', 'Kenya', 'Latonya', 'Latoya',\n",
    "           'Tamika', 'Tanisha']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "           'loyal','pleasure','diamond','gentle','honest','lucky','rainbow','diploma',\n",
    "           'gift','honor','miracle','sunrise','family','happy','laugher','paradise',\n",
    "           'vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy',\n",
    "          'bomb','divorce','jail','poverty','ugly','cancer','evil','kill',\n",
    "          'rotten','vomit']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [[str.lower(term) for term in terms] for terms in [X_terms, Y_terms, A_terms, B_terms]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 5\n",
    "X_terms = ['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil',\n",
    "'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith',\n",
    "           'Sarah']\n",
    "Y_terms = ['Darnell', 'Hakim', 'Jermaine', 'Kareem', 'Jamal', 'Leroy', 'Rasheed',\n",
    "'Tremayne', 'Tyrone', 'Aisha', 'Ebony', 'Keisha', 'Kenya', 'Latonya', 'Latoya',\n",
    "           'Tamika', 'Tanisha']\n",
    "A_terms = ['joy', 'love', 'peace', 'wonderful', 'pleasure', 'friend', 'laughter', 'happy']\n",
    "B_terms = ['agony', 'terrible', 'horrible', 'nasty', 'evil', 'war', 'awful', 'failure']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [[str.lower(term) for term in terms] for terms in [X_terms, Y_terms, A_terms, B_terms]]\n",
    "# Removed terms: \"Lakisha\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 6\n",
    "X_terms = ['John', 'Paul','Mike','Kevin','Steve','Greg','Jeff','Bill']\n",
    "Y_terms = ['Amy','Joan','Lisa','Sarah','Diana','Kate','Ann','Donna']\n",
    "A_terms = ['executive','management','professional','corporation',\n",
    "               'salary','office','business','career']\n",
    "B_terms = ['home','parents','children','family',\n",
    "               'cousins','marriage','wedding','relatives']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [tuple([str.lower(term) for term in terms]) for terms in [X_terms, Y_terms, A_terms, B_terms]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 7 using Google News word2vec model\n",
    "X_terms = ['math','algebra','geometry','calculus',\n",
    "             'equations','computation','numbers','addition']\n",
    "Y_terms = ['poetry','art','dance','literature',\n",
    "             'novel','symphony','drama','sculpture']\n",
    "A_terms = ['male','man','boy','brother',\n",
    "              'he','him','his','son']\n",
    "B_terms = ['female','woman','girl','sister',\n",
    "               'she','her','hers','daughter']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [[str.lower(term) for term in terms] for terms in [X_terms, Y_terms, A_terms, B_terms]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weat 8\n",
    "A_terms = ['brother','father','uncle','grandfather',\n",
    "          'son','he','his','him']\n",
    "B_terms = ['sister','mother','aunt','grandmother',\n",
    "          'daughter','she','hers','her']\n",
    "X_terms = ['science','technology','physics','chemistry',\n",
    "          'Einstein','NASA','experiment','astronomy']\n",
    "Y_terms = ['poetry','art','Shakespeare','dance',\n",
    "          'literature','novel','symphony','drama']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [[str.lower(term) for term in terms] for terms in [X_terms, Y_terms, A_terms, B_terms]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEAT 9\n",
    "X_terms = ['sad','hopeless','gloomy','tearful','miserable','depressed']\n",
    "Y_terms = ['sick','illness','influenza','disease','virus','cancer']\n",
    "A_terms = ['impermanent','unstable','variable','fleeting','short','brief',\n",
    "          'occasional']\n",
    "B_terms = ['stable','always','constant','persistent','chronic','prolonged','forever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weat 10\n",
    "X_terms = ['Tiffany', 'Michelle', 'Cindy', 'Kristy', 'Brad', 'Eric', 'Joey', 'Billy']\n",
    "Y_terms = ['Ethel', 'Bernice', 'Gertrude', 'Agnes', 'Cecil', 'Wilbert', 'Mortimer', 'Edgar']\n",
    "A_terms = ['joy', 'love', 'peace', 'wonderful', 'pleasure', 'friend', 'laughter', 'happy']\n",
    "B_terms = ['agony', 'terrible', 'horrible', 'nasty', 'evil', 'war', 'awful', 'failure']\n",
    "[X_terms, Y_terms, A_tersm, B_terms] = [[str.lower(term) for term in terms] for terms in [X_terms, Y_terms, A_terms, B_terms]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.815586923924345"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_1storder_effect_size_unnormalized(X_terms, Y_terms, A_terms, B_terms, we_model, E_ctx_vec, E_wrd_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Order Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.815586923924345\n",
      "['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n"
     ]
    }
   ],
   "source": [
    "print(produce_1storder_effect_size_unnormalized(X_terms, Y_terms, A_terms, B_terms, we_model, E_ctx_vec, E_wrd_vec))\n",
    "print(X_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 12870/12870.0 [1:21:29<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014114466527115344\n",
      "('john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill')\n",
      "Total time elapsed:  -4895.007618188858\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(produce_1storder_p_value(we_model, X_terms, Y_terms, A_terms, B_terms, E_ctx_vec, E_wrd_vec))\n",
    "end = time.time()\n",
    "print(X_terms)\n",
    "print ('Total time elapsed: ', start-end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5948170825435086"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_effect_size(we_model, X_terms, Y_terms, A_terms, B_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                      | 926/12870.0 [00:13<02:44, 72.62it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d0955e2534a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduce_2ndorder_p_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwe_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GRADSCHOOL\\SemesterProjects\\Navid\\GitRepos\\first-order-cooccurrence-metric-experiments\\src\\models\\effect_size.py\u001b[0m in \u001b[0;36mproduce_2ndorder_p_value\u001b[1;34m(wv_obj, X_terms, Y_terms, A_terms, B_terms)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_i_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_i_terms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_complements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_union_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_combinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_terms\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mtest_statistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduce_test_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_i_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_i_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_statistic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomparison_statistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mddof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GRADSCHOOL\\SemesterProjects\\Navid\\GitRepos\\first-order-cooccurrence-metric-experiments\\src\\models\\effect_size.py\u001b[0m in \u001b[0;36mproduce_test_statistic\u001b[1;34m(wv_obj, X_terms, Y_terms, A_terms, B_terms)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mproduce_test_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;34m'''Calculates test statistic s(X,Y,A,B).'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mX_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_mtx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_matrices_from_term_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mx_associations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx_vec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcalculate_association_metric_for_target_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0my_associations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0my_vec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcalculate_association_metric_for_target_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GRADSCHOOL\\SemesterProjects\\Navid\\GitRepos\\first-order-cooccurrence-metric-experiments\\src\\models\\effect_size.py\u001b[0m in \u001b[0;36mget_matrices_from_term_lists\u001b[1;34m(wv_obj, X_terms, Y_terms, A_terms, B_terms)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmatrices\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     the words in X_terms, Y_terms, A_terms, and B_terms.'''\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_set_to_mtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_association_metric_for_target_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GRADSCHOOL\\SemesterProjects\\Navid\\GitRepos\\first-order-cooccurrence-metric-experiments\\src\\models\\effect_size.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmatrices\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     the words in X_terms, Y_terms, A_terms, and B_terms.'''\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_set_to_mtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mterms\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_association_metric_for_target_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GRADSCHOOL\\SemesterProjects\\Navid\\GitRepos\\first-order-cooccurrence-metric-experiments\\src\\models\\effect_size.py\u001b[0m in \u001b[0;36mword_set_to_mtx\u001b[1;34m(wv_obj, word_set)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mword_set_to_mtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m'''Converts set of string words into a 2-D numpy array of word vectors from the word-vector object.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_matrices_from_term_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\semproject\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(produce_2ndorder_p_value(we_model, X_terms, Y_terms, A_terms, B_terms))\n",
    "print(X_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: 1.774, 1.595, 1.4183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1storder_association_metric_scaled(word, A_terms, B_terms):\n",
    "    A_relations = get_expSG_1storder_relation_scaled(word, A_terms)\n",
    "    B_relations = get_expSG_1storder_relation_scaled(word, B_terms)\n",
    "    return mean(A_relations.values()) - mean(B_relations.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4517385120046582"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def produce_1storder_effect_size_scaled(X_terms, Y_terms, A_terms, B_terms):\n",
    "    x_associations = np.array([])\n",
    "    y_associations = np.array([])\n",
    "    for (x,y) in zip(X_terms, Y_terms):\n",
    "        x_association = get_1storder_association_metric_scaled(x, A_terms, B_terms)\n",
    "        y_association = get_1storder_association_metric_scaled(y, A_terms, B_terms)\n",
    "        x_associations = np.append(x_associations, x_association)\n",
    "        y_associations = np.append(y_associations, y_association)\n",
    "    all_associations = np.append(x_associations, y_associations)\n",
    "    return (np.mean(x_associations) - np.mean(y_associations))/np.std(all_associations, ddof=1)\n",
    "produce_1storder_effect_size_scaled(X_terms, Y_terms, A_terms, B_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2nd order, 1st order unscaled, 1st order scaled)\n",
    "(1.774, 1.816, 1.759)\n",
    "(1.595, 1.648, 1.335)\n",
    "(1.4183, 1.471, 1.451)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "CacheInfo(hits=5, misses=4, maxsize=None, currsize=4)\n"
     ]
    }
   ],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def add_4(num):\n",
    "    return num+4\n",
    "\n",
    "lst = [1,1,2,2,3,3,3,4,4]\n",
    "for i in lst:\n",
    "    print(add_4(i))\n",
    "print(add_4.cache_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "tuple(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
