{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import filter_terms_not_in_wemodel, \\\n",
    "    get_2ndorder_association_metric_list_for_target_list, \\\n",
    "    get_1storder_association_metric_list_for_target_list, \\\n",
    "    get_expSG_1storder_relation_no_cache_NEW, \\\n",
    "    get_expSG_1storder_relation_no_cache_NEW_ALLWORDS, \\\n",
    "    get_matrices_from_term_lists, \\\n",
    "    save_array, open_pickle, save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n"
     ]
    }
   ],
   "source": [
    "we_model_name = \"sg_dim300_min100_win5\"\n",
    "we_vector_size = 300\n",
    "we_model_dir = '../data/external/wiki-english/wiki-english-20171001/%s' % we_model_name\n",
    "we_model = Word2Vec.load(we_model_dir+'/model.gensim')\n",
    "print ('loading done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = '../data/interim/association_metric_exps.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following terms were removed from the list first_list because they were not found in the we_model: ['bobbie-sue', 'sue-ellen']\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: ['terryl', 'aiesha', 'lashelle', 'temeka', 'tameisha', 'teretha', 'laronya', 'shanise', 'lakisha', 'sharise', 'tashika', 'lashandra', 'shavonn']\n",
      "The following terms were removed from the first list to balance the length of the lists: ['roger', 'alan', 'frank', 'ian', 'justin', 'ryan', 'andrew', 'fred', 'jack', 'matthew', 'stephen']\n",
      "The following terms were removed from the list first_list because they were not found in the we_model: []\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n"
     ]
    }
   ],
   "source": [
    "# WEAT 3\n",
    "EXP_NUM = 3\n",
    "X_terms = ['roger','alan','frank','ian','justin',\n",
    "          'ryan','andrew','fred','jack','matthew','stephen','brad','greg','jed',\n",
    "          'paul','todd','brandon','hank','jonathan','peter','wilbur','amanda',\n",
    "          'courtney','heather','melanie','sara','amber','crystal','katie',\n",
    "          'meredith','shannon','betsy','donna','kristin','nancy','stephanie',\n",
    "          'bobbie-sue','ellen','lauren','peggy','sue-ellen','colleen','emily',\n",
    "          'megan','rachel','wendy']\n",
    "Y_terms = ['alonzo','jamel','lerone','theo','alphonse','jerome','leroy',\n",
    "           'torrance','darnell','lamar','lionel','rashaun','tyree','deion',\n",
    "          'lamont','malik','terrence','tyrone','lavon','terryl',\n",
    "          'wardell','aiesha','lashelle','nichelle','shereen','temeka','ebony',\n",
    "          'latisha','shaniqua','tameisha','teretha','jasmine','laronya','shanise',\n",
    "          'tanisha','tia','lakisha','latoya','sharise','tashika','yolanda',\n",
    "          'lashandra','malika','shavonn','tawanda','yvette']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "          'loyal','pleasure','diamond','gentle','honest','lucky','rainbow',\n",
    "          'diploma','gift','honor','miracle','sunrise','family','happy','laughter',\n",
    "          'paradise','vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy','bomb',\n",
    "          'divorce','jail','poverty','ugly','cancer','evil','kill','rotten','vomit']\n",
    "X_terms, Y_terms = filter_terms_not_in_wemodel(we_model, X_terms, Y_terms)\n",
    "A_terms, B_terms = filter_terms_not_in_wemodel(we_model, A_terms, B_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1storder_association_metric_fast(word, A_terms, B_terms, we_model, scaler):\n",
    "    A_relations = get_expSG_1storder_relation_no_cache_NEW(word, A_terms, we_model)\n",
    "    B_relations = get_expSG_1storder_relation_no_cache_NEW(word, B_terms, we_model)\n",
    "    A_transformed = scaler.transform(A_relations.reshape(-1,1))\n",
    "    B_transformed = scaler.transform(B_relations.reshape(-1,1))\n",
    "    return np.mean(A_transformed) - np.mean(B_transformed)\n",
    "\n",
    "def get_all_relations_1storder(A_terms, B_terms, we_model):\n",
    "    A_relations=get_expSG_1storder_relation_no_cache_NEW_ALLWORDS(A_terms, we_model)\n",
    "    B_relations=get_expSG_1storder_relation_no_cache_NEW_ALLWORDS(B_terms, we_model)\n",
    "    A_associations = np.mean(A_relations, axis=1)\n",
    "    B_associations = np.mean(B_relations, axis=1)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler.fit(np.concatenate((A_associations, B_associations))) #concatenation\n",
    "    return all_associations, scaler\n",
    "\n",
    "def get_1storder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model):\n",
    "    global ORDER\n",
    "    ORDER = 'first'\n",
    "    all_associations, scaler = get_all_relations_1storder(A_terms, B_terms, we_model)\n",
    "    associations = np.array([])\n",
    "    for word in tqdm(target_list):\n",
    "        association = get_1storder_association_metric_fast(word, A_terms, B_terms, we_model, scaler)\n",
    "        associations = np.append(associations, association)\n",
    "    \n",
    "    return associations\n",
    "    #scaler.fit(all_associations.reshape(-1,1)) # Reshape is for a single feature, NOT for a single sample\n",
    "    #transformed = scaler.transform(associations.reshape(-1,1))\n",
    "    #return transformed.reshape(len(transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0.10841331 0.11889662 0.12435763 ... 0.00466087 0.01104624 0.00433315].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1beafec924c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_1storder_association_metric_list_for_target_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Make sure the last argument passed to save_array matches the first argument passed to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# get_1storder_association_metric_list_for_target_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msave_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILEPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXP_NUM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'first'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6e2be6446d8a>\u001b[0m in \u001b[0;36mget_1storder_association_metric_list_for_target_list\u001b[1;34m(target_list, A_terms, B_terms, we_model)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mORDER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mORDER\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'first'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mall_associations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_relations_1storder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_terms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwe_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0massociations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-6e2be6446d8a>\u001b[0m in \u001b[0;36mget_all_relations_1storder\u001b[1;34m(A_terms, B_terms, we_model)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_associations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB_associations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#concatenation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mall_associations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\semproject\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\semproject\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    371\u001b[0m         X = check_array(X,\n\u001b[0;32m    372\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\semproject\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0.10841331 0.11889662 0.12435763 ... 0.00466087 0.01104624 0.00433315].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "metrics = get_1storder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model)\n",
    "# Make sure the last argument passed to save_array matches the first argument passed to\n",
    "# get_1storder_association_metric_list_for_target_list\n",
    "save_array(FILEPATH, metrics, EXP_NUM, 'first', 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosines_for_target_word_unscaled(word_vec, A_mtx, B_mtx):\n",
    "    A_dot_v = np.dot(A_mtx, word_vec)\n",
    "    B_dot_v = np.dot(B_mtx, word_vec)\n",
    "    A_norms = np.multiply(np.linalg.norm(A_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    B_norms = np.multiply(np.linalg.norm(B_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    A_cosines = np.divide(A_dot_v, A_norms)\n",
    "    B_cosines = np.divide(B_dot_v, B_norms)\n",
    "    return np.mean(A_cosines), np.mean(B_cosines)\n",
    "\n",
    "def get_scaler(we_model, A_mtx, B_mtx):\n",
    "    '''Computes the association metric, s(w,A,B).\n",
    "    word_vec: 1-D word vector\n",
    "    A_mtx, B_mtx: 2-D word vector arrays'''\n",
    "    #A_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, A_mtx)\n",
    "    #B_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, B_mtx)\n",
    "    all_words_mtx = we_model.wv.vectors\n",
    "    all_associations_to_A = np.array([])\n",
    "    all_associations_to_B = np.array([])\n",
    "    for x_vec in tqdm(all_words_mtx, desc='Getting association metric for all words'):\n",
    "        A_cosine, B_cosine = calculate_cosines_for_target_word_unscaled(x_vec, A_mtx, B_mtx)\n",
    "        all_associations_to_A = np.append(all_associations_to_A, A_cosine)\n",
    "        all_associations_to_B = np.append(all_associations_to_B, B_cosine)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler.fit(np.concatenate((all_associations_to_A, all_associations_to_B)))\n",
    "    return scaler\n",
    "\n",
    "def calculate_association_metric_for_target_word_scaled(word_vec, A_mtx, B_mtx, scaler):\n",
    "    '''Computes the association metric, s(w,A,B).\n",
    "    word_vec: 1-D word vector\n",
    "    A_mtx, B_mtx: 2-D word vector arrays'''\n",
    "    #A_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, A_mtx)\n",
    "    #B_cosines_apply = np.apply_along_axis(lambda row: 1-cosine_distance(row, word_vec), 1, B_mtx)\n",
    "    A_dot_v = np.dot(A_mtx, word_vec)\n",
    "    B_dot_v = np.dot(B_mtx, word_vec)\n",
    "    A_norms = np.multiply(np.linalg.norm(A_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    B_norms = np.multiply(np.linalg.norm(B_mtx, axis=1), np.linalg.norm(word_vec))\n",
    "    A_cosines = np.divide(A_dot_v, A_norms)\n",
    "    B_cosines = np.divide(B_dot_v, B_norms)\n",
    "    A_transformed = scaler.transform(A_cosines.reshape(-1,1))\n",
    "    B_transformed = scaler.transform(B_cosines.reshape(-1,1))\n",
    "    return np.mean(A_transformed) - np.mean(B_transformed)\n",
    "\n",
    "def get_2ndorder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model):\n",
    "    global ORDER\n",
    "    ORDER = 'second'\n",
    "    \n",
    "    [X_mtx, _, A_mtx, B_mtx] = get_matrices_from_term_lists(we_model, target_list, target_list, A_terms, B_terms)\n",
    "    associations = np.apply_along_axis(lambda x_vec: calculate_association_metric_for_target_word(x_vec, A_mtx, B_mtx), 1, X_mtx)\n",
    "    \n",
    "    scaler = get_scaler(we_model, A_mtx, B_mtx)\n",
    "    \n",
    "    associations = np.array([])\n",
    "    for word in tqdm(target_list):\n",
    "        association = get_1storder_association_metric_fast(word, A_terms, B_terms, we_model, scaler)\n",
    "        associations = np.append(associations, association)\n",
    "    return associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting association metric for all words: 100%|██████████████████████████████| 312425/312425 [02:50<00:00, 1830.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results array successfully saved to file ../data/interim/association_metric_exps.pickle under keys [3]['second_order_X']\n"
     ]
    }
   ],
   "source": [
    "metrics = get_2ndorder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model)\n",
    "# Make sure the last argument passed to save_array matches the first argument passed to\n",
    "# get_1storder_association_metric_list_for_target_list\n",
    "save_array(FILEPATH, metrics, EXP_NUM, 'second', 'X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Results for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {3: {'first_order_X': array([0.10917358, 0.11447405, 0.03977664, 0.21711674, 0.02069784,\n",
       "                     0.1428209 , 0.23171016, 0.10893403, 0.22164942, 0.1731362 ,\n",
       "                     0.03183159, 0.08439055, 0.13934323, 0.11403528, 0.10248176,\n",
       "                     0.15985223, 0.36892206, 0.13024949, 0.11066183, 0.19458479,\n",
       "                     0.17395213, 0.18826508, 0.09246605, 0.14566013, 0.14104251,\n",
       "                     0.12317574, 0.17753855, 0.17748386, 0.11440848, 0.06771086,\n",
       "                     0.07495613, 0.06568201, 0.12313557]),\n",
       "              'second_order_X': array([0.23449796, 0.2345688 , 0.0989702 , 0.37882134, 0.02910594,\n",
       "                     0.27740997, 0.16379115, 0.19063637, 0.41707927, 0.29391664,\n",
       "                     0.1971741 , 0.21215774, 0.3162869 , 0.2859519 , 0.3024328 ,\n",
       "                     0.18131642, 0.4094162 , 0.2760841 , 0.23804706, 0.30866247,\n",
       "                     0.30504382, 0.30536607, 0.20394868, 0.3266883 , 0.28670394,\n",
       "                     0.314532  , 0.30773422, 0.38299367, 0.3626843 , 0.20989752,\n",
       "                     0.20443484, 0.2916711 , 0.32588214], dtype=float32)}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dict = open_pickle(FILEPATH)\n",
    "in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {3: {'first_order_X': array([0.10917358, 0.11447405, 0.03977664, 0.21711674, 0.02069784,\n",
       "                     0.1428209 , 0.23171016, 0.10893403, 0.22164942, 0.1731362 ,\n",
       "                     0.03183159, 0.08439055, 0.13934323, 0.11403528, 0.10248176,\n",
       "                     0.15985223, 0.36892206, 0.13024949, 0.11066183, 0.19458479,\n",
       "                     0.17395213, 0.18826508, 0.09246605, 0.14566013, 0.14104251,\n",
       "                     0.12317574, 0.17753855, 0.17748386, 0.11440848, 0.06771086,\n",
       "                     0.07495613, 0.06568201, 0.12313557]),\n",
       "              'second_order_X': array([0.23449796, 0.2345688 , 0.0989702 , 0.37882134, 0.02910594,\n",
       "                     0.27740997, 0.16379115, 0.19063637, 0.41707927, 0.29391664,\n",
       "                     0.1971741 , 0.21215774, 0.3162869 , 0.2859519 , 0.3024328 ,\n",
       "                     0.18131642, 0.4094162 , 0.2760841 , 0.23804706, 0.30866247,\n",
       "                     0.30504382, 0.30536607, 0.20394868, 0.3266883 , 0.28670394,\n",
       "                     0.314532  , 0.30773422, 0.38299367, 0.3626843 , 0.20989752,\n",
       "                     0.20443484, 0.2916711 , 0.32588214], dtype=float32)}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD RESULTS Before algorithm modification - don't delete\n",
    "in_dict = open_pickle(FILEPATH)\n",
    "in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23449796, 0.2345688 , 0.0989702 , 0.37882134, 0.02910594,\n",
       "       0.27740997, 0.16379115, 0.19063637, 0.41707927, 0.29391664,\n",
       "       0.1971741 , 0.21215774, 0.3162869 , 0.2859519 , 0.3024328 ,\n",
       "       0.18131642, 0.4094162 , 0.2760841 , 0.23804706, 0.30866247,\n",
       "       0.30504382, 0.30536607, 0.20394868, 0.3266883 , 0.28670394,\n",
       "       0.314532  , 0.30773422, 0.38299367, 0.3626843 , 0.20989752,\n",
       "       0.20443484, 0.2916711 , 0.32588214], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_NUM = 3\n",
    "EXP_NAME = 'second_order_X'\n",
    "arr_for_plotting = in_dict[EXP_NUM][EXP_NAME]\n",
    "arr_for_plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([[1],[2],[3]]).reshape(-1)\n",
    "arr2 = np.array([[4],[5],[6]]).reshape(-1)\n",
    "np.concatenate((arr1,arr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_save_file(filepath):\n",
    "    dct = defaultdict()\n",
    "    save_pickle(dct, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_save_file(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
