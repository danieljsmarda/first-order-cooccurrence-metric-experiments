{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from models import \\\n",
    "    get_expSG_1storder_relation_no_cache_NEW_ALLWORDS, \\\n",
    "    get_1storder_association_metric_fast, \\\n",
    "    get_matrices_from_term_lists, \\\n",
    "    calculate_association_metric_for_target_word, \\\n",
    "    word_set_to_mtx, save_pickle, open_pickle\n",
    "from models import filter_terms_not_in_wemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_model_name = \"sg_dim300_min100_win5\"\n",
    "we_vector_size = 300\n",
    "we_model_dir = '../data/external/wiki-english/wiki-english-20171001/%s' % we_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done!\n"
     ]
    }
   ],
   "source": [
    "we_model = Word2Vec.load(we_model_dir+'/model.gensim')\n",
    "\n",
    "print ('loading done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = '../data/interim/association_metric_exps.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating *d*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following terms were removed from the list first_list because they were not found in the we_model: ['bobbie-sue', 'sue-ellen']\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: ['terryl', 'aiesha', 'lashelle', 'temeka', 'tameisha', 'teretha', 'laronya', 'shanise', 'lakisha', 'sharise', 'tashika', 'lashandra', 'shavonn']\n",
      "The following terms were removed from the first list to balance the length of the lists: ['roger', 'alan', 'frank', 'ian', 'justin', 'ryan', 'andrew', 'fred', 'jack', 'matthew', 'stephen']\n",
      "The following terms were removed from the list first_list because they were not found in the we_model: []\n",
      "The following terms were removed from the list second_list because they were not found in the we_model: []\n"
     ]
    }
   ],
   "source": [
    "# WEAT 3\n",
    "EXP_NUM = 3\n",
    "X_terms = ['roger','alan','frank','ian','justin',\n",
    "          'ryan','andrew','fred','jack','matthew','stephen','brad','greg','jed',\n",
    "          'paul','todd','brandon','hank','jonathan','peter','wilbur','amanda',\n",
    "          'courtney','heather','melanie','sara','amber','crystal','katie',\n",
    "          'meredith','shannon','betsy','donna','kristin','nancy','stephanie',\n",
    "          'bobbie-sue','ellen','lauren','peggy','sue-ellen','colleen','emily',\n",
    "          'megan','rachel','wendy']\n",
    "Y_terms = ['alonzo','jamel','lerone','theo','alphonse','jerome','leroy',\n",
    "           'torrance','darnell','lamar','lionel','rashaun','tyree','deion',\n",
    "          'lamont','malik','terrence','tyrone','lavon','terryl',\n",
    "          'wardell','aiesha','lashelle','nichelle','shereen','temeka','ebony',\n",
    "          'latisha','shaniqua','tameisha','teretha','jasmine','laronya','shanise',\n",
    "          'tanisha','tia','lakisha','latoya','sharise','tashika','yolanda',\n",
    "          'lashandra','malika','shavonn','tawanda','yvette']\n",
    "A_terms = ['caress','freedom','health','love','peace','cheer','friend','heaven',\n",
    "          'loyal','pleasure','diamond','gentle','honest','lucky','rainbow',\n",
    "          'diploma','gift','honor','miracle','sunrise','family','happy','laughter',\n",
    "          'paradise','vacation']\n",
    "B_terms = ['abuse','crash','filth','murder','sickness','accident','death','grief',\n",
    "          'poison','stink','assault','disaster','hatred','pollute','tragedy','bomb',\n",
    "          'divorce','jail','poverty','ugly','cancer','evil','kill','rotten','vomit']\n",
    "X_terms, Y_terms = filter_terms_not_in_wemodel(we_model, X_terms, Y_terms)\n",
    "A_terms, B_terms = filter_terms_not_in_wemodel(we_model, A_terms, B_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating List of Association Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_relations(A_terms, B_terms, we_model):\n",
    "    A_relations=get_expSG_1storder_relation_no_cache_NEW_ALLWORDS(A_terms, we_model)\n",
    "    B_relations=get_expSG_1storder_relation_no_cache_NEW_ALLWORDS(B_terms, we_model)\n",
    "    all_associations = np.mean(A_relations, axis=1) - np.mean(B_relations, axis=1)\n",
    "    return all_associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 2113.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10917358 0.114474   0.03977669 0.21711674 0.02069786 0.14282087\n",
      " 0.23171016 0.10893398 0.22164942 0.1731362  0.03183164 0.0843906\n",
      " 0.13934326 0.11403528 0.10248176 0.15985223 0.36892195 0.13024954\n",
      " 0.11066178 0.19458481 0.17395213 0.18826511 0.09246607 0.14566008\n",
      " 0.14104248 0.12317581 0.17753852 0.17748386 0.11440853 0.06771086\n",
      " 0.07495608 0.06568201 0.12313557]\n"
     ]
    }
   ],
   "source": [
    "def get_1storder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model):\n",
    "    global ORDER\n",
    "    ORDER = 'first'\n",
    "    \n",
    "    associations = np.array([])\n",
    "    for word in tqdm(target_list):\n",
    "        association = get_1storder_association_metric_fast(word, A_terms, B_terms, we_model)\n",
    "        associations = np.append(associations, association)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    all_associations = get_all_relations(A_terms, B_terms, we_model)\n",
    "    scaler.fit(all_associations.reshape(-1,1)) # Reshape is for a single feature, NOT for a single sample\n",
    "    transformed = scaler.transform(associations.reshape(-1,1))\n",
    "    return transformed.reshape(len(transformed))\n",
    "metrics = get_1storder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model)\n",
    "save_array(FILEPATH, metrics, EXP_NUM, ORDER)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting association metric for all words:  23%|██████▉                       | 71686/312425 [00:05<00:21, 11138.53it/s]"
     ]
    }
   ],
   "source": [
    "def get_2ndorder_association_metric_list_for_target_list(target_list, A_terms, B_terms, we_model):\n",
    "    global ORDER\n",
    "    ORDER = 'second'\n",
    "    \n",
    "    [X_mtx, _, A_mtx, B_mtx] = get_matrices_from_term_lists(we_model, target_list, target_list, A_terms, B_terms)\n",
    "    associations = np.apply_along_axis(lambda x_vec: calculate_association_metric_for_target_word(x_vec, A_mtx, B_mtx), 1, X_mtx)\n",
    "    \n",
    "    all_words_mtx = we_model.wv.vectors\n",
    "    all_associations = np.array([])\n",
    "    for x_vec in tqdm(all_words_mtx, desc='Getting association metric for all words'):\n",
    "        metric = calculate_association_metric_for_target_word(x_vec, A_mtx, B_mtx)\n",
    "        all_associations = np.append(all_associations, metric)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    scaler.fit(all_associations.reshape(-1,1)) # Reshape is for a single feature, NOT for a single sample\n",
    "    transformed = scaler.transform(associations.reshape(-1,1))\n",
    "    return transformed.reshape(len(transformed))\n",
    "metric = get_2ndorder_association_metric_list_for_target_list(X_terms, A_terms, B_terms, we_model)\n",
    "save_array(FILEPATH, metrics, EXP_NUM, ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]]\n",
      "[[3 3 3]\n",
      " [4 4 4]]\n",
      "[1 1 1]\n",
      "[3 3 3]\n",
      "next\n",
      "[2 2 2]\n",
      "[4 4 4]\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,1,1],[2,2,2]])\n",
    "B = np.array([[3,3,3],[4,4,4]])\n",
    "print(A)\n",
    "print(B)\n",
    "for (a,b) in zip(A,B):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print('next')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23606798, 2.23606798, 2.23606798])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (2,3) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8c6a83198eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mA_dot_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,3) and (2,3) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "A_dot_B = np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = defaultdict(dict)\n",
    "save_pickle(results_dict, FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {3: {'first': array([0.10917358, 0.114474  , 0.03977669, 0.21711674, 0.02069786,\n",
       "                     0.14282087, 0.23171016, 0.10893398, 0.22164942, 0.1731362 ,\n",
       "                     0.03183164, 0.0843906 , 0.13934326, 0.11403528, 0.10248176,\n",
       "                     0.15985223, 0.36892195, 0.13024954, 0.11066178, 0.19458481,\n",
       "                     0.17395213, 0.18826511, 0.09246607, 0.14566008, 0.14104248,\n",
       "                     0.12317581, 0.17753852, 0.17748386, 0.11440853, 0.06771086,\n",
       "                     0.07495608, 0.06568201, 0.12313557])}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_dict = open_pickle(FILEPATH)\n",
    "in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(FILEPATH, arr, exp_num, order):\n",
    "    results_dict = open_pickle(FILEPATH)\n",
    "    results_dict[exp_num][order] = arr\n",
    "    save_pickle(results_dict, FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
